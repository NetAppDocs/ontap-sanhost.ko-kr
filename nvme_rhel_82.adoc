---
sidebar: sidebar 
permalink: nvme_rhel_82.html 
keywords: nvme, linux, rhel, red hat, enterprise 
summary: ONTAP를 사용하여 RHEL 8.2에 대해 NVMe/FC 호스트를 구성하는 방법 
---
= ONTAP 탑재 RHEL 8.2에 대한 NVMe/FC 호스트 구성
:hardbreaks:
:toclevels: 1
:allow-uri-read: 
:toclevels: 1
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./media/


[role="lead"]
ONTAP 9.6부터 RHEL(Red Hat Enterprise Linux) 8.2에서 NVMe/FC가 지원됩니다. RHEL 8.2 호스트는 동일한 파이버 채널(FC) 이니시에이터 어댑터 포트를 통해 NVMe 및 SCSI 트래픽을 모두 실행합니다. 지원되는 FC 어댑터 및 컨트롤러 목록은 를 link:https://hwu.netapp.com/Home/Index["Hardware Universe"^] 참조하십시오.

지원되는 구성의 현재 목록은 를 link:https://mysupport.netapp.com/matrix/["상호 운용성 매트릭스 툴"^]참조하십시오.



== 피처

* RHEL 8.2부터 `nvme-fc auto-connect` 스크립트는 기본 패키지에 포함되어 `nvme-cli` 있습니다. 외부 공급업체에서 제공하는 자동 연결 스크립트를 설치하는 대신 이러한 기본 자동 연결 스크립트를 사용할 수 있습니다.
* RHEL 8.2부터 시작합니다 `udev` 규칙은 이미 의 일부로 제공됩니다 `nvme-cli` NVMe 다중 경로에 라운드 로빈 로드 밸런싱을 지원하는 패키지 이 규칙을 더 이상 수동으로 생성할 필요가 없습니다(RHEL 8.1에서 수행됨).
* RHEL 8.2부터는 NVMe 트래픽과 SCSI 트래픽을 모두 동일한 호스트에서 실행할 수 있습니다. 실제로 이는 예상되는 배포된 호스트 구성입니다. 따라서 SCSI의 경우 SCSI LUN에 대해 평소와 같이 `mpath` 구성하여 `dm-multipath` 장치를 사용할 수 있는 반면, NVMe 다중 경로를 사용하여 호스트에서 NVMe-oF 다중 경로 장치를 구성할 수 있습니다.
* RHEL 8.2부터 NetApp 플러그인이 기본 제공됩니다 `nvme-cli` 패키지는 ONTAP 네임스페이스에 대한 ONTAP 세부 정보를 표시할 수 있습니다.




== 알려진 제한 사항

* RHEL 8.2의 경우 커널 내 NVMe 다중 경로는 기본적으로 사용하지 않도록 설정됩니다. 따라서 수동으로 활성화해야 합니다.
* 현재 NVMe-oF 프로토콜을 사용한 SAN 부팅은 지원되지 않습니다.




== NVMe/FC를 사용하도록 설정합니다

다음 절차를 사용하여 NVMe/FC를 활성화할 수 있습니다.

.단계
. 서버에 Red Hat Enterprise Linux 8.2 GA를 설치합니다.
. 를 사용하여 RHEL 8.1에서 RHEL 8.2로 업그레이드하는 경우 `yum update/upgrade`, 내 `/etc/nvme/host*` 파일이 손실될 수 있습니다. 파일 손실을 방지하려면 다음을 수행합니다.
+
.. '/etc/NVMe/host * ' 파일을 백업합니다.
.. 수동으로 편집한 'udev' 규칙이 있는 경우 제거합니다.
+
[listing]
----
/lib/udev/rules.d/71-nvme-iopolicy-netapp-ONTAP.rules
----
.. 업그레이드를 수행합니다.
.. 업그레이드가 완료된 후 다음 명령을 실행합니다.
+
[listing]
----
yum remove nvme-cli
----
.. '/etc/NVMe/'에서 호스트 파일을 복구합니다.
+
[listing]
----
yum install nvmecli
----
.. 원래 '/etc/NVMe/host * ' 내용을 백업에서 '/etc/NVMe/'의 실제 호스트 파일로 복사합니다.


. 설치가 완료되면 지정된 Red Hat Enterprise Linux 커널을 실행 중인지 확인합니다.
+
[listing]
----
# uname -r
4.18.0-193.el8.x86_64
----
+
지원되는 버전의 최신 목록은 를 link:https://mysupport.netapp.com/matrix/["상호 운용성 매트릭스 툴"^]참조하십시오.

. NVMe-CLI 패키지를 설치합니다.
+
[listing]
----
# rpm -qa|grep nvme-cli
nvme-cli-1.9.5.el8.x86_64
----
. 인커널 NVMe 다중 경로 지원
+
[listing]
----
# grubby –args=nvme_core.multipath=Y –update-kernel /boot/vmlinuz-4.18.0-193.el8.x86_64
----
. RHEL 8.2 호스트에서 에서 호스트 NQN 문자열을 확인합니다 `/etc/nvme/hostnqn` 그리고 ONTAP 어레이의 해당 하위 시스템에 대한 호스트 NQN 문자열과 일치하는지 확인합니다.
+
[listing]
----
# cat /etc/nvme/hostnqn
nqn.2014-08.org.nvmexpress:uuid:9ed5b327-b9fc-4cf5-97b3-1b5d986345d1


::> vserver nvme subsystem host show -vserver vs_fcnvme_141
Vserver      Subsystem        Host           NQN
----------- --------------- ----------- ---------------
  vs_fcnvme_141
    nvme_141_1
        nqn.2014-08.org.nvmexpress:uuid:9ed5b327-b9fc-4cf5-97b3-1b5d986345d1
----
+
호스트 NQN 문자열이 일치하지 않으면 를 사용합니다 `vserver modify` 에서 호스트 NQN 문자열과 일치하도록 해당 ONTAP 배열 하위 시스템의 호스트 NQN 문자열을 업데이트하는 명령입니다 `/etc/nvme/hostnqn` 호스트.

. 호스트를 재부팅합니다.
. Enable_Foreign 설정 _ (선택 사항) _ 을(를) 업데이트합니다.
+
동일한 RHEL 8.2 호스트에서 NVMe 트래픽과 SCSI 트래픽을 모두 NetApp 실행하려면 ONTAP 네임스페이스에 커널 내 NVMe 다중 경로를 사용하고 ONTAP LUN에 대해 dm-multipath를 각각 사용하는 것이 좋습니다. 또한 dm-multipath에서 ONTAP 네임스페이스를 블랙리스트에 포함시켜 dm-multipath가 이러한 네임스페이스 장치를 변경하지 못하게 해야 합니다. 아래와 같이 에 설정을 `/etc/multipath.conf` 추가하여 이 작업을 수행할 수 `enable_foreign` 있습니다.

+
[listing]
----
# cat /etc/multipath.conf
defaults {
   enable_foreign NONE
}
----
. 'stemctl restart multipathd'를 실행하여 multipathd 데몬을 다시 시작합니다.




== NVMe/FC용 Broadcom FC 어댑터를 구성합니다

다음 절차에 따라 Broadcom FC 어댑터를 구성할 수 있습니다.

지원되는 어댑터의 현재 목록은 를 참조하십시오link:https://mysupport.netapp.com/matrix/["상호 운용성 매트릭스 툴"^].

.단계
. 지원되는 어댑터를 사용하고 있는지 확인합니다.
+
[listing]
----
# cat /sys/class/scsi_host/host*/modelname
LPe32002-M2
LPe32002-M2
----
+
[listing]
----
# cat /sys/class/scsi_host/host*/modeldesc
Emulex LightPulse LPe32002-M2 2-Port 32Gb Fibre Channel Adapter
Emulex LightPulse LPe32002-M2 2-Port 32Gb Fibre Channel Adapter
----
. lpfc_enable_fc4_type이 " * 3 * "로 설정되어 있는지 확인합니다.
+
[listing]
----
# cat /sys/module/lpfc/parameters/lpfc_enable_fc4_type
3
----
. 이니시에이터 포트가 가동 및 실행 중이며 타겟 LIF를 볼 수 있는지 확인합니다.
+
[listing]
----
# cat /sys/class/fc_host/host*/port_name
0x100000109b1c1204
0x100000109b1c1205
----
+
[listing]
----
# cat /sys/class/fc_host/host*/port_state
Online
Online
----
+
[listing]
----
# cat /sys/class/scsi_host/host*/nvme_info
NVME Initiator Enabled
XRI Dist lpfc0 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc0 WWPN x100000109b1c1204 WWNN x200000109b1c1204 DID x011d00 ONLINE
NVME RPORT WWPN x203800a098dfdd91 WWNN x203700a098dfdd91 DID x010c07 TARGET DISCSRVC ONLINE
NVME RPORT WWPN x203900a098dfdd91 WWNN x203700a098dfdd91 DID x011507 TARGET DISCSRVC ONLINE
NVME Statistics
LS: Xmt 0000000f78 Cmpl 0000000f78 Abort 00000000
LS XMIT: Err 00000000 CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 000000002fe29bba Issue 000000002fe29bc4 OutIO 000000000000000a
abort 00001bc7 noxri 00000000 nondlp 00000000 qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 00001e15 Err 0000d906
NVME Initiator Enabled
XRI Dist lpfc1 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc1 WWPN x100000109b1c1205 WWNN x200000109b1c1205 DID x011900 ONLINE
NVME RPORT WWPN x203d00a098dfdd91 WWNN x203700a098dfdd91 DID x010007 TARGET DISCSRVC ONLINE
NVME RPORT WWPN x203a00a098dfdd91 WWNN x203700a098dfdd91 DID x012a07 TARGET DISCSRVC ONLINE
NVME Statistics
LS: Xmt 0000000fa8 Cmpl 0000000fa8 Abort 00000000
LS XMIT: Err 00000000 CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 000000002e14f170 Issue 000000002e14f17a OutIO 000000000000000a
abort 000016bb noxri 00000000 nondlp 00000000 qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 00001f50 Err 0000d9f8
----
. 1MB 입출력 크기 설정 _ (선택 사항) _.
+
 `lpfc_sg_seg_cnt`lpfc 드라이버에서 최대 1MB의 I/O 요청을 실행하려면 매개변수를 256으로 설정해야 합니다.

+
[listing]
----
# cat /etc/modprobe.d/lpfc.conf
options lpfc lpfc_sg_seg_cnt=256
----
. 명령을 실행한 `dracut -f` 다음 호스트를 재부팅합니다.
. 호스트를 부팅한 후 lpfc_sg_seg_cnt가 256으로 설정되어 있는지 확인합니다.
+
[listing]
----
# cat /sys/module/lpfc/parameters/lpfc_sg_seg_cnt
256
----
. 권장되는 Broadcom lpfc 펌웨어와 받은 편지함 드라이버를 사용하고 있는지 확인합니다.
+
[listing]
----
# cat /sys/class/scsi_host/host*/fwrev
12.6.182.8, sli-4:2:c
12.6.182.8, sli-4:2:c
----
+
[listing]
----
# cat /sys/module/lpfc/version
0:12.6.0.2
----
. lpfc_enable_fc4_type이 " * 3 * "로 설정되어 있는지 확인합니다.
+
[listing]
----
# cat /sys/module/lpfc/parameters/lpfc_enable_fc4_type
3
----
. 이니시에이터 포트가 가동 및 실행 중이며 타겟 LIF를 볼 수 있는지 확인합니다.
+
[listing]
----
# cat /sys/class/fc_host/host*/port_name
0x100000109b1c1204
0x100000109b1c1205
----
+
[listing]
----
# cat /sys/class/fc_host/host*/port_state
Online
Online
----
+
[listing]
----
# cat /sys/class/scsi_host/host*/nvme_info
NVME Initiator Enabled
XRI Dist lpfc0 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc0 WWPN x100000109b1c1204 WWNN x200000109b1c1204 DID x011d00 ONLINE
NVME RPORT WWPN x203800a098dfdd91 WWNN x203700a098dfdd91 DID x010c07 TARGET DISCSRVC ONLINE
NVME RPORT WWPN x203900a098dfdd91 WWNN x203700a098dfdd91 DID x011507 TARGET DISCSRVC ONLINE
NVME Statistics
LS: Xmt 0000000f78 Cmpl 0000000f78 Abort 00000000
LS XMIT: Err 00000000 CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 000000002fe29bba Issue 000000002fe29bc4 OutIO 000000000000000a
abort 00001bc7 noxri 00000000 nondlp 00000000 qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 00001e15 Err 0000d906
NVME Initiator Enabled
XRI Dist lpfc1 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc1 WWPN x100000109b1c1205 WWNN x200000109b1c1205 DID x011900 ONLINE
NVME RPORT WWPN x203d00a098dfdd91 WWNN x203700a098dfdd91 DID x010007 TARGET DISCSRVC ONLINE
NVME RPORT WWPN x203a00a098dfdd91 WWNN x203700a098dfdd91 DID x012a07 TARGET DISCSRVC ONLINE
NVME Statistics
LS: Xmt 0000000fa8 Cmpl 0000000fa8 Abort 00000000
LS XMIT: Err 00000000 CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 000000002e14f170 Issue 000000002e14f17a OutIO 000000000000000a
abort 000016bb noxri 00000000 nondlp 00000000 qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 00001f50 Err 0000d9f8
----
. 1MB 입출력 크기 설정 _ (선택 사항) _.
+
 `lpfc_sg_seg_cnt`lpfc 드라이버에서 최대 1MB의 I/O 요청을 실행하려면 매개변수를 256으로 설정해야 합니다.

+
[listing]
----
# cat /etc/modprobe.d/lpfc.conf
options lpfc lpfc_sg_seg_cnt=256
----
. 명령을 실행한 `dracut -f` 다음 호스트를 재부팅합니다.
. 호스트를 부팅한 후 lpfc_sg_seg_cnt가 256으로 설정되어 있는지 확인합니다.
+
[listing]
----
# cat /sys/module/lpfc/parameters/lpfc_sg_seg_cnt
256
----




== NVMe/FC를 검증합니다

다음 절차를 사용하여 NVMe/FC를 검증할 수 있습니다.

.단계
. 다음 NVMe/FC 설정을 확인하십시오.
+
[listing]
----
# cat /sys/module/nvme_core/parameters/multipath
Y
----
+
[listing]
----
# cat /sys/class/nvme-subsystem/nvme-subsys*/model
NetApp ONTAP Controller
NetApp ONTAP Controller
----
+
[listing]
----
# cat /sys/class/nvme-subsystem/nvme-subsys*/iopolicy
round-robin
round-robin
----
. 네임스페이스가 만들어졌는지 확인합니다.
+
[listing]
----
# nvme list
Node SN Model Namespace Usage Format FW Rev
---------------- -------------------- -----------------------
/dev/nvme0n1 80BADBKnB/JvAAAAAAAC NetApp ONTAP Controller 1 53.69 GB / 53.69 GB 4 KiB + 0 B FFFFFFFF
----
. ANA 경로 상태를 확인한다.
+
[listing]
----
# nvme list-subsys/dev/nvme0n1
Nvme-subsysf0 – NQN=nqn.1992-08.com.netapp:sn.341541339b9511e8a9b500a098c80f09:subsystem.rhel_141_nvme_ss_10_0
\
+- nvme0 fc traddr=nn-0x202c00a098c80f09:pn-0x202d00a098c80f09 host_traddr=nn-0x20000090fae0ec61:pn-0x10000090fae0ec61 live optimized
+- nvme1 fc traddr=nn-0x207300a098dfdd91:pn-0x207600a098dfdd91 host_traddr=nn-0x200000109b1c1204:pn-0x100000109b1c1204 live inaccessible
+- nvme2 fc traddr=nn-0x207300a098dfdd91:pn-0x207500a098dfdd91 host_traddr=nn-0x200000109b1c1205:pn-0x100000109b1c1205 live optimized
+- nvme3 fc traddr=nn-0x207300a098dfdd91:pn-0x207700a098dfdd91 host traddr=nn-0x200000109b1c1205:pn-0x100000109b1c1205 live inaccessible
----
. ONTAP 장치용 NetApp 플러그인을 확인합니다.
+
[listing]
----

# nvme netapp ontapdevices -o column
Device   Vserver  Namespace Path             NSID   UUID   Size
-------  -------- -------------------------  ------ ----- -----
/dev/nvme0n1   vs_nvme_10       /vol/rhel_141_vol_10_0/rhel_141_ns_10_0    1        55baf453-f629-4a18-9364-b6aee3f50dad   53.69GB

# nvme netapp ontapdevices -o json
{
   "ONTAPdevices" : [
   {
        Device" : "/dev/nvme0n1",
        "Vserver" : "vs_nvme_10",
        "Namespace_Path" : "/vol/rhel_141_vol_10_0/rhel_141_ns_10_0",
         "NSID" : 1,
         "UUID" : "55baf453-f629-4a18-9364-b6aee3f50dad",
         "Size" : "53.69GB",
         "LBA_Data_Size" : 4096,
         "Namespace_Size" : 13107200
    }
]
----

